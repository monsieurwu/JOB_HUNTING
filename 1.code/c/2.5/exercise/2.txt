asd智能技术的飞速发展，将大语言模型(LLMs)部署到边缘设备上已成为当前 AI 领域的一个热门趋势。这一趋势不仅体现在微软 Windows 11 AI + PC 等产品中，也反映了整个行业对增强设备本地智能的追求。然而，在资源受限的端侧设备上部署庞大的语言模型并非易事，这需要在模型性能和硬件限制之间寻求平衡。

回顾过去几年的发展历程，我们可以看到端侧 AI 部署经历了几个关键阶段：

1. 早期尝试(2019-2020)：在 ChatGPT 都还没问世的那些年，研究人员主要关注如何将较小的神经网络模型（如 MobileNet）部署到移动设备上。这个阶段的主要挑战是如何在有限的计算资源下保持模型的准确性。

2. 量化技术的兴起(2020-2021)：随着模型规模的增大，量化技术开始受到广泛关注。Google 的研究团队在 TensorFlow Lite 框架中引入了动态范围量化，而 ARM 则推出了用于神经网络推理的 CMSIS-NN 库。

3. 大语言模型的挑战(2022-至今)：随着大语言模型的出现，如何在端侧设备上部署这些庞大的模型成为了新的挑战。苹果、谷歌、华为等科技巨头都在积极探索将 LLMs 部署到移动设备的方法。例如，苹果在iOS 17 中引入了本地运行的语言模型，而谷歌则在 Pixel 设备上部署了 Gemini Nano 模型，实现了部分 AI 功能的本地化。

以消费者的视角观察，今年 9 月的苹果发布会，可能就是端侧 AI 的第一次大规模面世了。但问题接踵而至：从舆情的角度来看，国行 iPhone 怎么办？不用 iPhone 和 Pixel 手机的人要怎么体验端侧大模型？从技术的角度来看，目前部署的大语言模型多会量化到低比特。而低比特 LLMs 在推理过程中需要进行低精度权重和高精度激活向量的混合精度矩阵乘法（mpGEMM）。

现有的系统由于硬件缺乏对 mpGEMM 的原生支持，不得不将权重反量化以进行高精度计算。这种间接的方式导致了显著的推理开销，并且无法随着比特数进一步降低而获得加速。

面对这些挑战，一些研究者开始思考：是否存在一种方法，可以同时解决低比特计算、混合精度问题，并且不依赖专用硬件？这种思考引导他们将目光投向了一个看似简单却颇具潜力的方向：查找表（Look-Up Table，LUT）。

查找表是计算机科学中一个古老而基础的概念。通过预先计算并存储结果，查找表可以将复杂的计算转化为简单的内存访问，这在某些情况下可以显著提高计算速度。那么，这个概念是否可以应
————————————————

                            版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。
                        
原文链接：https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/141177078# 2024-8-15 17:20:23 # 1th run of the program #
# 2024-8-15 17:20:25 # 1th run of the program #
# 2024-8-15 17:20:31 # 1th run of the program #
sdsd# 2024-8-15 17:20:38 # 1th run of the program #
# 2024-8-15 17:20:41 # 1th run of the program #
# 2024-8-15 17:20:42 # 1th run of the program #
# 2024-8-15 17:20:42 # 1th run of the program #
# 2024-8-15 17:20:43 # 1th run of the program #
232
2323

# 2024-8-15 17:20:43 # 1th run of the program #
# 2024-8-15 17:22:3 # 1th run of the program #
# 2024-8-15 17:22:3 # 1th run of the program #
# 2024-8-15 17:22:4 # 1th run of the program #
# 2024-8-15 17:22:5 # 1th run of the program #
# 2024-8-16 16:22:33 # 1th run of the program #
# 2024-8-16 16:22:36 # 1th run of the program #
# 2024-8-16 16:23:10 # 1th run of the program #
# 2024-8-16 16:23:11 # 1th run of the program #
# 2024-8-16 16:23:12 # 1th run of the program #
# 2024-8-16 16:23:36 # 1th run of the program #
# 2024-8-16 16:23:37 # 1th run of the program #
# 2024-8-16 16:23:38 # 1th run of the program #
# 2024-8-16 16:24:26 # 1th run of the program #
# 2024-8-16 16:24:27 # 1th run of the program #
# 2024-8-16 16:24:28 # 1th run of the program #
# 2024-8-16 16:24:29 # 1th run of the program #
# 2024-8-16 16:24:29 # 1th run of the program #
# 2024-8-16 16:24:30 # 1th run of the program #
# 2024-8-16 16:24:30 # 1th run of the program #
# 2024-8-16 16:24:30 # 1th run of the program #
